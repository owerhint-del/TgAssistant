"""
–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–º–µ—Ç–æ–∫ —Å URL-–∞–º–∏.

–§–æ—Ä–º–∞—Ç –∑–∞–º–µ—Ç–∫–∏:
  –¢–µ–º–∞ (–ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –±–µ–∑ URL)
  ‚û°Ô∏è –æ–ø–∏—Å–∞–Ω–∏–µ https://url1.com
  üëâ https://url2.com - –æ–ø–∏—Å–∞–Ω–∏–µ
  —Ç–µ–∫—Å—Ç –±–µ–∑ URL ‚Üí –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è

–†–µ–∑—É–ª—å—Ç–∞—Ç: ParsedNote —Å —Ç–µ–º–æ–π, –≥—Ä—É–ø–ø–∞–º–∏ –∏ –ø–ª–æ—Å–∫–∏–º —Å–ø–∏—Å–∫–æ–º –∑–∞–ø–∏—Å–µ–π.
"""
import re
import unicodedata
from dataclasses import dataclass, field
from typing import Optional

from app.utils.url_parser import parse_url, ParsedLink

# URL –≤ —Ç–µ–∫—Å—Ç–µ
_URL_RE = re.compile(r"https?://\S+")

# –ò–∑–≤–µ—Å—Ç–Ω—ã–µ emoji-–ø—Ä–µ—Ñ–∏–∫—Å—ã –≥—Ä—É–ø–ø (—Ä–∞—Å—à–∏—Ä—è–µ–º—ã–π —Å–ø–∏—Å–æ–∫)
_EMOJI_PREFIXES = frozenset({
    "\u27a1\ufe0f",  # ‚û°Ô∏è
    "\u27a1",        # ‚û° (–±–µ–∑ variation selector)
    "\U0001f449",    # üëâ
    "\u2764\ufe0f",  # ‚ù§Ô∏è
    "\u2764",        # ‚ù§
    "\U0001f525",    # üî•
    "\u2b50",        # ‚≠ê
    "\U0001f4cc",    # üìå
    "\U0001f4a1",    # üí°
    "\U0001f3af",    # üéØ
    "\u2705",        # ‚úÖ
    "\U0001f4ce",    # üìé
    "\U0001f4e2",    # üì¢
    "\U0001f680",    # üöÄ
    "\u26a1",        # ‚ö°
})

# –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Ç—Ä–µ–ª–∫–∏ –∫–∞–∫ –≥—Ä—É–ø–ø–æ–≤—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
_ARROW_PREFIXES = ("->", "=>", "-->")


@dataclass
class NoteEntry:
    url: str                        # —Å—ã—Ä–æ–π URL
    label: str                      # –æ–ø–∏—Å–∞–Ω–∏–µ
    group: str                      # –∫–ª—é—á –≥—Ä—É–ø–ø—ã ("" = –±–µ–∑ –≥—Ä—É–ø–ø—ã)
    line_number: int                # –¥–ª—è –æ—Ç—á—ë—Ç–∞ –æ–± –æ—à–∏–±–∫–∞—Ö
    link: Optional[ParsedLink] = None  # —Ä–µ–∑—É–ª—å—Ç–∞—Ç parse_url(), None –µ—Å–ª–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π


@dataclass
class NoteGroup:
    prefix: str                     # –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã–π –ø—Ä–µ—Ñ–∏–∫—Å
    entries: list[NoteEntry] = field(default_factory=list)


@dataclass
class ParsedNote:
    topic: str
    groups: list[NoteGroup] = field(default_factory=list)
    entries: list[NoteEntry] = field(default_factory=list)  # –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫
    skipped_lines: list[tuple[int, str]] = field(default_factory=list)
    errors: list[tuple[int, str, str]] = field(default_factory=list)  # (line, url, error_msg)

    @property
    def valid_count(self) -> int:
        """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π —Å —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–º URL."""
        return sum(1 for e in self.entries if e.link is not None)

    @property
    def total_count(self) -> int:
        return len(self.entries)


def slugify(text: str, max_length: int = 50) -> str:
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –≤ slug: lowercase, –Ω–µ-alnum ‚Üí _, trim.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—É (—Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è –Ω–µ –¥–µ–ª–∞–µ—Ç—Å—è ‚Äî –ø—Ä–æ—Å—Ç–æ lowercase).
    """
    # Normalize unicode
    text = unicodedata.normalize("NFKD", text)
    # Lowercase
    text = text.lower()
    # Replace non-alphanumeric (including unicode letters) with _
    text = re.sub(r"[^\w]", "_", text, flags=re.UNICODE)
    # Collapse multiple underscores
    text = re.sub(r"_+", "_", text)
    # Strip leading/trailing underscores
    text = text.strip("_")
    # Truncate
    if len(text) > max_length:
        text = text[:max_length].rstrip("_")
    return text or "untitled"


_EMOJI_PREFIXES_SORTED = sorted(_EMOJI_PREFIXES, key=len, reverse=True)


def _detect_group_prefix(text: str) -> Optional[str]:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥—Ä—É–ø–ø–æ–≤–æ–π –ø—Ä–µ—Ñ–∏–∫—Å –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ URL –∏–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å –∏–ª–∏ None.
    """
    text = text.strip()
    if not text:
        return None

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º emoji-–ø—Ä–µ—Ñ–∏–∫—Å—ã (–¥–ª–∏–Ω–Ω—ã–µ –ø–µ—Ä–≤—ã–º–∏, —á—Ç–æ–±—ã ‚û°Ô∏è —Å–æ–≤–ø–∞–ª–æ —Ä–∞–Ω—å—à–µ ‚û°)
    for emoji in _EMOJI_PREFIXES_SORTED:
        if text.startswith(emoji):
            return emoji

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Ç—Ä–µ–ª–∫–∏
    for arrow in _ARROW_PREFIXES:
        if text.startswith(arrow):
            return arrow

    return None


def _extract_label(text: str, url: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ —Å—Ç—Ä–æ–∫–∏, —É–±–∏—Ä–∞—è URL –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: 'desc - url', 'url - desc', 'desc url', 'url desc'.
    """
    # –£–±–∏—Ä–∞–µ–º URL
    remaining = text.replace(url, "", 1).strip()

    # –£–±–∏—Ä–∞–µ–º –≥—Ä—É–ø–ø–æ–≤–æ–π –ø—Ä–µ—Ñ–∏–∫—Å –µ—Å–ª–∏ –µ—Å—Ç—å
    prefix = _detect_group_prefix(remaining)
    if prefix:
        remaining = remaining[len(prefix):].strip()

    # –£–±–∏—Ä–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Å –∫—Ä–∞—ë–≤
    for sep in (" - ", " | ", " ‚Äî ", " ‚Äì "):
        remaining = remaining.strip()
        if remaining.startswith(sep.strip()):
            remaining = remaining[len(sep.strip()):].strip()
        if remaining.endswith(sep.strip()):
            remaining = remaining[:-(len(sep.strip()))].strip()

    return remaining.strip()


def parse_note(text: str) -> ParsedNote:
    """
    –ü–∞—Ä—Å–∏—Ç –∑–∞–º–µ—Ç–∫—É ‚Üí ParsedNote.

    1. –¢–µ–º–∞: –ø–µ—Ä–≤–∞—è –Ω–µ–ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –±–µ–∑ URL (–∏–ª–∏ "Untitled").
    2. –ó–∞–ø–∏—Å–∏: —Å—Ç—Ä–æ–∫–∏ —Å URL, —Å –≥—Ä—É–ø–ø–∞–º–∏ –ø–æ emoji/—Å—Ç—Ä–µ–ª–∫–∞–º.
    3. –°—Ç—Ä–æ–∫–∏ –±–µ–∑ URL ‚Üí skipped_lines.
    """
    lines = text.split("\n")

    topic = ""
    entries: list[NoteEntry] = []
    skipped: list[tuple[int, str]] = []
    errors: list[tuple[int, str, str]] = []
    current_group = ""
    topic_found = False

    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫)
    groups_dict: dict[str, NoteGroup] = {}

    for i, raw_line in enumerate(lines, start=1):
        line = raw_line.strip()
        if not line:
            continue

        url_match = _URL_RE.search(line)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É –∏–∑ –ø–µ—Ä–≤–æ–π —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ URL
        if not topic_found:
            if not url_match:
                # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –±–µ–∑ URL ‚Äî —Ç–µ–º–∞
                topic = line.lstrip("#").strip()
                topic_found = True
                continue
            else:
                # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç URL ‚Äî —Ç–µ–º–∞ = Untitled
                topic = "Untitled"
                topic_found = True
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —ç—Ç–æ–π —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ –∑–∞–ø–∏—Å–∏

        if not url_match:
            # –°—Ç—Ä–æ–∫–∞ –±–µ–∑ URL ‚Äî –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä—É–ø–ø—ã
            prefix = _detect_group_prefix(line)
            if prefix:
                current_group = prefix
                # –°–æ–∑–¥–∞—ë–º –≥—Ä—É–ø–ø—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                if current_group not in groups_dict:
                    groups_dict[current_group] = NoteGroup(prefix=current_group)
                continue
            # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ URL ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            skipped.append((i, line))
            continue

        url = url_match.group(0)
        # –ß–∏—Å—Ç–∏–º URL –æ—Ç trailing –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        while url and url[-1] in (")", "]", ",", ";", ".", "!"):
            url = url[:-1]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—É –¥–ª—è —ç—Ç–æ–π —Å—Ç—Ä–æ–∫–∏
        text_before_url = line[:url_match.start()]
        line_prefix = _detect_group_prefix(text_before_url)
        if line_prefix:
            current_group = line_prefix

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
        label = _extract_label(line, url_match.group(0))

        # –í–∞–ª–∏–¥–∞—Ü–∏—è URL —á–µ—Ä–µ–∑ parse_url
        link: Optional[ParsedLink] = None
        try:
            link = parse_url(url)
        except ValueError as e:
            errors.append((i, url, str(e)))

        entry = NoteEntry(
            url=url,
            label=label,
            group=current_group,
            line_number=i,
            link=link,
        )
        entries.append(entry)

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≥—Ä—É–ø–ø—É
        group_key = current_group
        if group_key not in groups_dict:
            groups_dict[group_key] = NoteGroup(prefix=group_key or "BASE")
        groups_dict[group_key].entries.append(entry)

    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    if not topic:
        topic = "Untitled"

    groups = list(groups_dict.values())

    return ParsedNote(
        topic=topic,
        groups=groups,
        entries=entries,
        skipped_lines=skipped,
        errors=errors,
    )
